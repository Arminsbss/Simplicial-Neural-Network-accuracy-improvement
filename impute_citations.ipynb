{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcmDkSkg58Zv//ukt9weiz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arminsbss/Simplicial-Neural-Network/blob/main/impute_citations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "!git clone https://github.com/stefaniaebli/simplicial_neural_networks.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeBWqxeTByZA",
        "outputId": "76e36275-db5f-448e-ace5-9150984eb934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'simplicial_neural_networks'...\n",
            "remote: Enumerating objects: 339, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 339 (delta 50), reused 48 (delta 48), pack-reused 280\u001b[K\n",
            "Receiving objects: 100% (339/339), 2.42 MiB | 1.71 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/simplicial_neural_networks/scnn\n",
        "# from simplicial_neural_networks import scnn\n",
        "import simplicial_neural_networks.scnn\n",
        "import simplicial_neural_networks.scnn.chebyshev\n"
      ],
      "metadata": {
        "id": "qMWXG34_UAt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title scnn\n",
        "#scnn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import simplicial_neural_networks.scnn.chebyshev\n",
        "def coo2tensor(A):\n",
        "    assert(sp.isspmatrix_coo(A))\n",
        "    idxs = torch.LongTensor(np.vstack((A.row, A.col)))\n",
        "    vals = torch.FloatTensor(A.data)\n",
        "    return torch.sparse_coo_tensor(idxs, vals, size = A.shape, requires_grad = False)\n",
        "\n",
        "class SimplicialConvolution(nn.Module):\n",
        "    def __init__(self, K, C_in, C_out, enable_bias = True, variance = 1.0, groups = 1):\n",
        "        assert groups == 1, \"Only groups = 1 is currently supported.\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert(C_in > 0)\n",
        "        assert(C_out > 0)\n",
        "        assert(K > 0)\n",
        "\n",
        "        self.C_in = C_in\n",
        "        self.C_out = C_out\n",
        "        self.K = K\n",
        "        self.enable_bias = enable_bias\n",
        "\n",
        "        self.theta = nn.parameter.Parameter(variance*torch.randn((self.C_out, self.C_in, self.K)))\n",
        "        if self.enable_bias:\n",
        "            self.bias = nn.parameter.Parameter(torch.zeros((1, self.C_out, 1)))\n",
        "        else:\n",
        "            self.bias = 0.0\n",
        "\n",
        "    def forward(self, L, x):\n",
        "        assert(len(L.shape) == 2)\n",
        "        assert(L.shape[0] == L.shape[1])\n",
        "\n",
        "        (B, C_in, M) = x.shape\n",
        "\n",
        "        assert(M == L.shape[0])\n",
        "        assert(C_in == self.C_in)\n",
        "\n",
        "        X = scnn.chebyshev.assemble(self.K, L, x)\n",
        "        y = torch.einsum(\"bimk,oik->bom\", (X, self.theta))\n",
        "        assert(y.shape == (B, self.C_out, M))\n",
        "\n",
        "        return y + self.bias\n",
        "\n",
        "# This class does not yet implement the\n",
        "# Laplacian-power-pre/post-composed with the coboundary. It can be\n",
        "# simulated by just adding more layers anyway, so keeping it simple\n",
        "# for now.\n",
        "#\n",
        "# Note: You can use this for a adjoints of coboundaries too. Just feed\n",
        "# a transposed D.\n",
        "class Coboundary(nn.Module):\n",
        "    def __init__(self, C_in, C_out, enable_bias = True, variance = 1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        assert(C_in > 0)\n",
        "        assert(C_out > 0)\n",
        "\n",
        "        self.C_in = C_in\n",
        "        self.C_out = C_out\n",
        "        self.enable_bias = enable_bias\n",
        "\n",
        "        self.theta = nn.parameter.Parameter(variance*torch.randn((self.C_out, self.C_in)))\n",
        "        if self.enable_bias:\n",
        "            self.bias = nn.parameter.Parameter(torch.zeros((1, self.C_out, 1)))\n",
        "        else:\n",
        "            self.bias = 0.0\n",
        "\n",
        "    def forward(self, D, x):\n",
        "        assert(len(D.shape) == 2)\n",
        "\n",
        "        (B, C_in, M) = x.shape\n",
        "\n",
        "        assert(D.shape[1] == M)\n",
        "        assert(C_in == self.C_in)\n",
        "\n",
        "        N = D.shape[0]\n",
        "\n",
        "        # This is essentially the equivalent of chebyshev.assemble for\n",
        "        # the convolutional modules.\n",
        "        X = []\n",
        "        for b in range(0, B):\n",
        "            X12 = []\n",
        "            for c_in in range(0, self.C_in):\n",
        "                X12.append(D.mm(x[b, c_in, :].unsqueeze(1)).transpose(0,1)) # D.mm(x[b, c_in, :]) has shape Nx1\n",
        "            X12 = torch.cat(X12, 0)\n",
        "\n",
        "            assert(X12.shape == (self.C_in, N))\n",
        "            X.append(X12.unsqueeze(0))\n",
        "\n",
        "        X = torch.cat(X, 0)\n",
        "        assert(X.shape == (B, self.C_in, N))\n",
        "\n",
        "        y = torch.einsum(\"oi,bin->bon\", (self.theta, X))\n",
        "        assert(y.shape == (B, self.C_out, N))\n",
        "\n",
        "        return y + self.bias"
      ],
      "metadata": {
        "id": "_0wznF4nD_vA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title chebyshef\n",
        "#chebyshef\n",
        "import torch\n",
        "import scipy.sparse as sp\n",
        "import scipy.sparse.linalg as spl\n",
        "import numpy as np\n",
        "def normalize(L, half_interval = False):\n",
        "    assert(sp.isspmatrix(L))\n",
        "    M = L.shape[0]\n",
        "    assert(M == L.shape[1])\n",
        "    topeig = spl.eigsh(L, k=1, which=\"LM\", return_eigenvectors = False)[0]\n",
        "    #print(\"Topeig = %f\" %(topeig))\n",
        "\n",
        "    ret = L.copy()\n",
        "    if half_interval:\n",
        "        ret *= 1.0/topeig\n",
        "    else:\n",
        "        ret *= 2.0/topeig\n",
        "        ret.setdiag(ret.diagonal(0) - np.ones(M), 0)\n",
        "\n",
        "    return ret\n",
        "\n",
        "def assemble(K, L, x):\n",
        "    (B, C_in, M) = x.shape\n",
        "    assert(L.shape[0] == M)\n",
        "    assert(L.shape[0] == L.shape[1])\n",
        "    assert(K > 0)\n",
        "\n",
        "    X = []\n",
        "    for b in range(0, B):\n",
        "        X123 = []\n",
        "        for c_in in range(0, C_in):\n",
        "            X23 = []\n",
        "            X23.append(x[b, c_in, :].unsqueeze(1)) # Constant, k = 0 term.\n",
        "\n",
        "            if K > 1:\n",
        "                X23.append(L.mm(X23[0]))\n",
        "            for k in range(2, K):\n",
        "                X23.append(2*(L.mm(X23[k-1])) - X23[k-2])\n",
        "\n",
        "            X23 = torch.cat(X23, 1)\n",
        "            assert(X23.shape == (M, K))\n",
        "            X123.append(X23.unsqueeze(0))\n",
        "\n",
        "        X123 = torch.cat(X123, 0)\n",
        "        assert(X123.shape == (C_in, M, K))\n",
        "        X.append(X123.unsqueeze(0))\n",
        "\n",
        "    X = torch.cat(X, 0)\n",
        "    assert(X.shape == (B, C_in, M, K))\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "i1w5OlG9EQiX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MySCNN\n",
        "\n",
        "class MySCNN(nn.Module):\n",
        "    def __init__(self, colors = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        assert(colors > 0)\n",
        "        self.colors = colors\n",
        "\n",
        "        num_filters = 30 #20\n",
        "        variance = 0.01 #0.001\n",
        "\n",
        "        # Degree 0 convolutions.\n",
        "        self.C0_1 = scnn.scnn.SimplicialConvolution(5, self.colors, num_filters*self.colors, variance=variance)\n",
        "        self.C0_2 = scnn.scnn.SimplicialConvolution(5, num_filters*self.colors, num_filters*self.colors, variance=variance)\n",
        "        self.C0_3 = scnn.scnn.SimplicialConvolution(5, num_filters*self.colors, self.colors, variance=variance)\n",
        "\n",
        "        # Degree 1 convolutions.\n",
        "        self.C1_1 = scnn.scnn.SimplicialConvolution(5, self.colors, num_filters*self.colors, variance=variance)\n",
        "        self.C1_2 = scnn.scnn.SimplicialConvolution(5, num_filters*self.colors, num_filters*self.colors, variance=variance)\n",
        "        self.C1_3 = scnn.scnn.SimplicialConvolution(5, num_filters*self.colors, self.colors, variance=variance)\n",
        "\n",
        "        # Degree 2 convolutions.\n",
        "        self.C2_1 = scnn.scnn.SimplicialConvolution(5, self.colors, num_filters*self.colors, variance=variance)\n",
        "        self.C2_2 = scnn.scnn.SimplicialConvolution(5, num_filters*self.colors, num_filters*self.colors, variance=variance)\n",
        "        self.C2_3 = scnn.scnn.SimplicialConvolution(5, num_filters*self.colors, self.colors, variance=variance)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, Ls, Ds, adDs, xs):\n",
        "        assert(len(xs) == 3) # The three degrees are fed together as a list.\n",
        "\n",
        "        assert(len(Ls) == len(Ds))\n",
        "        Ms = [L.shape[0] for L in Ls]\n",
        "        Ns = [D.shape[0] for D in Ds]\n",
        "\n",
        "        Bs = [x.shape[0] for x in xs]\n",
        "        C_ins = [x.shape[1] for x in xs]\n",
        "        Ms = [x.shape[2] for x in xs]\n",
        "\n",
        "        assert(Ms == [D.shape[1] for D in Ds])\n",
        "        assert(Ms == [L.shape[1] for L in Ls])\n",
        "        assert([adD.shape[0] for adD in adDs] == [D.shape[1] for D in Ds])\n",
        "        assert([adD.shape[1] for adD in adDs] == [D.shape[0] for D in Ds])\n",
        "\n",
        "        assert(Bs == len(Bs)*[Bs[0]])\n",
        "        assert(C_ins == len(C_ins)*[C_ins[0]])\n",
        "\n",
        "        out0_1 = self.C0_1(Ls[0], xs[0]) #+ self.D10_1(xs[1])\n",
        "        out1_1 = self.C1_1(Ls[1], xs[1]) #+ self.D01_1(xs[0]) + self.D21_1(xs[2])\n",
        "        out2_1 = self.C2_1(Ls[2], xs[2]) #+ self.D12_1(xs[1])\n",
        "\n",
        "        out0_2 = self.C0_2(Ls[0], nn.LeakyReLU()(out0_1)) #+ self.D10_2(nn.LeakyReLU()(out1_1))\n",
        "        out1_2 = self.C1_2(Ls[1], nn.LeakyReLU()(out1_1)) #+ self.D01_2(nn.LeakyReLU()(out0_1)) + self.D21_2(nn.LeakyReLU()(out2_1))\n",
        "        out2_2 = self.C2_2(Ls[2], nn.LeakyReLU()(out2_1)) #+ self.D12_2(nn.LeakyReLU()(out1_1))\n",
        "\n",
        "        out0_3 = self.C0_3(Ls[0], nn.LeakyReLU()(out0_2)) #+ self.D10_3(nn.LeakyReLU()(out1_2))\n",
        "        out1_3 = self.C1_3(Ls[1], nn.LeakyReLU()(out1_2)) #+ self.D01_3(nn.LeakyReLU()(out0_2)) + self.D21_2(nn.LeakyReLU()(out2_2))\n",
        "        out2_3 = self.C2_3(Ls[2], nn.LeakyReLU()(out2_2)) #+ self.D12_3(nn.LeakyReLU()(out1_2))\n",
        "\n",
        "        #return [out0_3, torch.zeros_like(xs[1]), torch.zeros_like(xs[2])]\n",
        "        #return [torch.zeros_like(xs[0]), out1_3, torch.zeros_like(xs[2])]\n",
        "        return [out0_3, out1_3, out2_3]"
      ],
      "metadata": {
        "id": "AU5tY376B18q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('.')"
      ],
      "metadata": {
        "id": "e3Ka3WpgZsJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/simplicial_neural_networks/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hrg9l-T1jze",
        "outputId": "73cfcf9f-dcf8-402c-92a3-6d3b9f719539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/simplicial_neural_networks/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logdir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "WIV2-OfZFSL2",
        "outputId": "10708dc4-7186-4706-a875-972039975737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5cd57e36e8d8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'logdir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/simplicial_neural_networks/data/s2_3_collaboration_complex\n",
        "laplacians = np.load('150250_laplacians.npy',allow_pickle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37D6K1Qxhlka",
        "outputId": "6f3ea986-a8bc-4e3f-81b3-a98d63752c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/simplicial_neural_networks/data/s2_3_collaboration_complex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(laplacians)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5deVrDVTAT-",
        "outputId": "b643871b-8b29-4f30-c0d7-6e0dab7339a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<352x352 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 3300 stored elements in COOrdinate format>\n",
            " <1474x1474 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 25466 stored elements in COOrdinate format>\n",
            " <3285x3285 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 4697 stored elements in COOrdinate format>\n",
            " <5019x5019 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 5419 stored elements in COOrdinate format>\n",
            " <5559x5559 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 5601 stored elements in COOrdinate format>\n",
            " <4547x4547 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 4547 stored elements in COOrdinate format>\n",
            " <2732x2732 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 2732 stored elements in COOrdinate format>\n",
            " <1175x1175 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 1175 stored elements in COOrdinate format>\n",
            " <343x343 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 343 stored elements in COOrdinate format>\n",
            " <61x61 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 61 stored elements in COOrdinate format>\n",
            " <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
            " \twith 5 stored elements in COOrdinate format>       ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(laplacians[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRI9_HpE6eoC",
        "outputId": "92eaaf33-df24-42b7-957a-be82f4a1705e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t6.0\n",
            "  (0, 63)\t-1.0\n",
            "  (0, 64)\t-1.0\n",
            "  (0, 65)\t-1.0\n",
            "  (0, 66)\t-1.0\n",
            "  (0, 67)\t-1.0\n",
            "  (0, 68)\t-1.0\n",
            "  (0, 69)\t-1.0\n",
            "  (0, 70)\t-1.0\n",
            "  (0, 71)\t-1.0\n",
            "  (0, 72)\t-1.0\n",
            "  (0, 73)\t-1.0\n",
            "  (0, 74)\t-1.0\n",
            "  (0, 75)\t-1.0\n",
            "  (0, 76)\t-1.0\n",
            "  (0, 77)\t-1.0\n",
            "  (0, 78)\t-1.0\n",
            "  (0, 79)\t-1.0\n",
            "  (0, 80)\t-1.0\n",
            "  (0, 81)\t-1.0\n",
            "  (0, 82)\t-1.0\n",
            "  (0, 83)\t-1.0\n",
            "  (0, 84)\t-1.0\n",
            "  (0, 85)\t-1.0\n",
            "  (0, 86)\t-1.0\n",
            "  :\t:\n",
            "  (1471, 1471)\t11.0\n",
            "  (1471, 139)\t1.0\n",
            "  (1471, 470)\t1.0\n",
            "  (1471, 555)\t1.0\n",
            "  (1471, 804)\t1.0\n",
            "  (1471, 1201)\t1.0\n",
            "  (1471, 1357)\t1.0\n",
            "  (1471, 1373)\t1.0\n",
            "  (1471, 1408)\t1.0\n",
            "  (1471, 1473)\t-1.0\n",
            "  (1472, 1472)\t7.0\n",
            "  (1473, 29)\t-1.0\n",
            "  (1473, 154)\t-1.0\n",
            "  (1473, 470)\t-1.0\n",
            "  (1473, 555)\t-1.0\n",
            "  (1473, 771)\t-1.0\n",
            "  (1473, 804)\t-1.0\n",
            "  (1473, 1087)\t-1.0\n",
            "  (1473, 1213)\t-1.0\n",
            "  (1473, 1294)\t-1.0\n",
            "  (1473, 1320)\t-1.0\n",
            "  (1473, 1373)\t-1.0\n",
            "  (1473, 1408)\t-1.0\n",
            "  (1473, 1471)\t-1.0\n",
            "  (1473, 1473)\t7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the last one\n",
        "print(laplacians[1].getrow(1471).getcol(1409).toarray()[0][0])\n",
        "if int(laplacians[1].getrow(1471).getcol(1409).toarray()[0][0])==0.0:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "1H1-Jm7tR3tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb4b5328-2656-408c-e43c-95d7f2b451f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "1472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "laplacians[1].getrow(1471).getcol(1409).toarray()[0][0]==0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-xIAij3_rLJ",
        "outputId": "965c2702-d585-412f-e806-06a6ae9cf4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(laplacians[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v4LvzL2ud0r",
        "outputId": "8b4fdff6-df60-4596-9966-1b10f8efc6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._coo.coo_matrix'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "laplacians[1].data[(laplacians[1].row==1471) & (laplacians[1].col==1409)]=9.0"
      ],
      "metadata": {
        "id": "FQRlRQpcsOti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1473):\n",
        "  print(laplacians[1].getrow(i).getcol(i).toarray()[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTEAiLm48Tqo",
        "outputId": "aa68df31-4857-422b-ce2c-ecc9bdffac95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "5.0\n",
            "5.0\n",
            "5.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "9.0\n",
            "7.0\n",
            "4.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "7.0\n",
            "5.0\n",
            "2.0\n",
            "3.0\n",
            "5.0\n",
            "7.0\n",
            "9.0\n",
            "4.0\n",
            "7.0\n",
            "4.0\n",
            "8.0\n",
            "7.0\n",
            "9.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "2.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "6.0\n",
            "9.0\n",
            "6.0\n",
            "8.0\n",
            "9.0\n",
            "6.0\n",
            "9.0\n",
            "7.0\n",
            "8.0\n",
            "9.0\n",
            "6.0\n",
            "7.0\n",
            "6.0\n",
            "6.0\n",
            "8.0\n",
            "7.0\n",
            "6.0\n",
            "8.0\n",
            "6.0\n",
            "7.0\n",
            "9.0\n",
            "3.0\n",
            "7.0\n",
            "9.0\n",
            "8.0\n",
            "7.0\n",
            "9.0\n",
            "5.0\n",
            "7.0\n",
            "7.0\n",
            "6.0\n",
            "7.0\n",
            "8.0\n",
            "6.0\n",
            "5.0\n",
            "7.0\n",
            "8.0\n",
            "6.0\n",
            "6.0\n",
            "7.0\n",
            "7.0\n",
            "8.0\n",
            "5.0\n",
            "7.0\n",
            "6.0\n",
            "7.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "5.0\n",
            "5.0\n",
            "5.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "4.0\n",
            "6.0\n",
            "9.0\n",
            "6.0\n",
            "6.0\n",
            "7.0\n",
            "6.0\n",
            "6.0\n",
            "7.0\n",
            "6.0\n",
            "4.0\n",
            "7.0\n",
            "6.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "7.0\n",
            "7.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "7.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "4.0\n",
            "9.0\n",
            "5.0\n",
            "5.0\n",
            "5.0\n",
            "5.0\n",
            "5.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "4.0\n",
            "4.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "5.0\n",
            "5.0\n",
            "5.0\n",
            "5.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "4.0\n",
            "4.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "5.0\n",
            "5.0\n",
            "5.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "5.0\n",
            "5.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "3.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "8.0\n",
            "6.0\n",
            "10.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "6.0\n",
            "6.0\n",
            "8.0\n",
            "8.0\n",
            "10.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "5.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "11.0\n",
            "11.0\n",
            "15.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "15.0\n",
            "11.0\n",
            "15.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "5.0\n",
            "9.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "15.0\n",
            "14.0\n",
            "5.0\n",
            "9.0\n",
            "7.0\n",
            "9.0\n",
            "5.0\n",
            "9.0\n",
            "16.0\n",
            "14.0\n",
            "9.0\n",
            "9.0\n",
            "13.0\n",
            "9.0\n",
            "9.0\n",
            "11.0\n",
            "11.0\n",
            "9.0\n",
            "7.0\n",
            "24.0\n",
            "12.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "4.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "5.0\n",
            "5.0\n",
            "5.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "4.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "5.0\n",
            "5.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "9.0\n",
            "5.0\n",
            "9.0\n",
            "9.0\n",
            "7.0\n",
            "15.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "14.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "14.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "5.0\n",
            "5.0\n",
            "5.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "5.0\n",
            "5.0\n",
            "7.0\n",
            "7.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "14.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "6.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "7.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "5.0\n",
            "9.0\n",
            "9.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "9.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "11.0\n",
            "11.0\n",
            "9.0\n",
            "11.0\n",
            "11.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "6.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "11.0\n",
            "11.0\n",
            "13.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "7.0\n",
            "7.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "8.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "6.0\n",
            "6.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "6.0\n",
            "7.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "8.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "16.0\n",
            "10.0\n",
            "13.0\n",
            "7.0\n",
            "11.0\n",
            "8.0\n",
            "8.0\n",
            "6.0\n",
            "8.0\n",
            "10.0\n",
            "8.0\n",
            "7.0\n",
            "10.0\n",
            "13.0\n",
            "10.0\n",
            "8.0\n",
            "10.0\n",
            "8.0\n",
            "10.0\n",
            "8.0\n",
            "7.0\n",
            "6.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "10.0\n",
            "10.0\n",
            "7.0\n",
            "8.0\n",
            "8.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "8.0\n",
            "8.0\n",
            "7.0\n",
            "6.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "7.0\n",
            "11.0\n",
            "7.0\n",
            "11.0\n",
            "9.0\n",
            "7.0\n",
            "8.0\n",
            "6.0\n",
            "10.0\n",
            "8.0\n",
            "10.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "7.0\n",
            "13.0\n",
            "9.0\n",
            "7.0\n",
            "9.0\n",
            "7.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "7.0\n",
            "6.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "5.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "6.0\n",
            "9.0\n",
            "8.0\n",
            "7.0\n",
            "8.0\n",
            "6.0\n",
            "6.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "10.0\n",
            "8.0\n",
            "6.0\n",
            "10.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "10.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "6.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "5.0\n",
            "8.0\n",
            "8.0\n",
            "10.0\n",
            "8.0\n",
            "6.0\n",
            "7.0\n",
            "7.0\n",
            "11.0\n",
            "11.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "7.0\n",
            "9.0\n",
            "11.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "15.0\n",
            "7.0\n",
            "10.0\n",
            "10.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "10.0\n",
            "11.0\n",
            "11.0\n",
            "8.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "6.0\n",
            "6.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "11.0\n",
            "10.0\n",
            "8.0\n",
            "7.0\n",
            "7.0\n",
            "7.0\n",
            "6.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "6.0\n",
            "10.0\n",
            "11.0\n",
            "7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse as sp\n",
        "\n",
        "# create new coordinates and values\n",
        "new_coords = [(1471, 1409)]\n",
        "new_values = [4.0]\n",
        "\n",
        "# find the indices of the existing coordinates\n",
        "idx = laplacians[1].row * laplacians[1].shape[1] + laplacians[1].col\n",
        "\n",
        "# find the indices of the new coordinates\n",
        "new_idx = [coord[0] * laplacians[1].shape[1] + coord[1] for coord in new_coords]\n",
        "\n",
        "# find the indices where the new coordinates should be inserted\n",
        "insert_idx = sorted(list(set(new_idx) - set(idx)))\n",
        "\n",
        "# create new coordinate and value arrays\n",
        "new_row = laplacians[1].row.tolist() + [coord[0] for coord in new_coords]\n",
        "new_col = laplacians[1].col.tolist() + [coord[1] for coord in new_coords]\n",
        "new_data = laplacians[1].data.tolist() + new_values\n",
        "\n",
        "# insert the new coordinates and values into the existing matrix\n",
        "laplacians[1] = sp.coo_matrix((new_data, (new_row, new_col)), shape=laplacians[1].shape)\n",
        "\n",
        "# sort the matrix by row and column indices\n",
        "laplacians[1] = sp.coo_matrix((laplacians[1].data[np.argsort(idx.tolist() + insert_idx)],\n",
        "                               (laplacians[1].row[np.argsort(idx.tolist() + insert_idx)],\n",
        "                                laplacians[1].col[np.argsort(idx.tolist() + insert_idx)])),\n",
        "                              shape=laplacians[1].shape)\n"
      ],
      "metadata": {
        "id": "3sgiQ8RE7a40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(laplacians[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g4MjSeunpXl",
        "outputId": "0038b1da-5e85-4ec0-8f9e-3715fdae616f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._coo.coo_matrix'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "laplacians[1].data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8J124qKlAif",
        "outputId": "06870413-44c1-455c-d09b-8514d0770ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "laplacians[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1CEC-G0SK-7",
        "outputId": "0a215995-6e77-4d03-ad3a-81495e58c7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1474, 1474)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random_number = random.random()\n",
        "print(random_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNTkSXic45Zs",
        "outputId": "206c931c-d49d-4b69-c201-2c5739403a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.20771364427792316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch._C import float32\n",
        "for i in range(25466):\n",
        "  laplacians[1].data[i] *= random_number\n"
      ],
      "metadata": {
        "id": "sgZm42Y6h5tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1474):\n",
        "  laplacians[1].data[(laplacians[1].row==i) & (laplacians[1].col==i)]+=1\n"
      ],
      "metadata": {
        "id": "xBXoGE6uB_ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(laplacians[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWsak96PmiHJ",
        "outputId": "96922b73-8d0a-4215-ab1f-c422a8c8b19b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t2.2462819\n",
            "  (0, 63)\t-0.20771365\n",
            "  (0, 64)\t-0.20771365\n",
            "  (0, 65)\t-0.20771365\n",
            "  (0, 66)\t-0.20771365\n",
            "  (0, 67)\t-0.20771365\n",
            "  (0, 68)\t-0.20771365\n",
            "  (0, 69)\t-0.20771365\n",
            "  (0, 70)\t-0.20771365\n",
            "  (0, 71)\t-0.20771365\n",
            "  (0, 72)\t-0.20771365\n",
            "  (0, 73)\t-0.20771365\n",
            "  (0, 74)\t-0.20771365\n",
            "  (0, 75)\t-0.20771365\n",
            "  (0, 76)\t-0.20771365\n",
            "  (0, 77)\t-0.20771365\n",
            "  (0, 78)\t-0.20771365\n",
            "  (0, 79)\t-0.20771365\n",
            "  (0, 80)\t-0.20771365\n",
            "  (0, 81)\t-0.20771365\n",
            "  (0, 82)\t-0.20771365\n",
            "  (0, 83)\t-0.20771365\n",
            "  (0, 84)\t-0.20771365\n",
            "  (0, 85)\t-0.20771365\n",
            "  (0, 86)\t-0.20771365\n",
            "  :\t:\n",
            "  (1471, 1471)\t3.2848501\n",
            "  (1471, 139)\t0.20771365\n",
            "  (1471, 470)\t0.20771365\n",
            "  (1471, 555)\t0.20771365\n",
            "  (1471, 804)\t0.20771365\n",
            "  (1471, 1201)\t0.20771365\n",
            "  (1471, 1357)\t0.20771365\n",
            "  (1471, 1373)\t0.20771365\n",
            "  (1471, 1408)\t0.20771365\n",
            "  (1471, 1473)\t-0.20771365\n",
            "  (1472, 1472)\t2.4539955\n",
            "  (1473, 29)\t-0.20771365\n",
            "  (1473, 154)\t-0.20771365\n",
            "  (1473, 470)\t-0.20771365\n",
            "  (1473, 555)\t-0.20771365\n",
            "  (1473, 771)\t-0.20771365\n",
            "  (1473, 804)\t-0.20771365\n",
            "  (1473, 1087)\t-0.20771365\n",
            "  (1473, 1213)\t-0.20771365\n",
            "  (1473, 1294)\t-0.20771365\n",
            "  (1473, 1320)\t-0.20771365\n",
            "  (1473, 1373)\t-0.20771365\n",
            "  (1473, 1408)\t-0.20771365\n",
            "  (1473, 1471)\t-0.20771365\n",
            "  (1473, 1473)\t2.4539955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/simplicial_neural_networks\n",
        "np.save('150250_laplacians.npy', laplacians)\n"
      ],
      "metadata": {
        "id": "cGJiKYhvkG67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4334ed51-fc7e-463b-8e43-477422c22bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/simplicial_neural_networks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse.linalg import inv\n",
        "laplacians[1]=inv(laplacians[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZCVpHIg6WD9",
        "outputId": "503ee0c4-4776-4d33-921b-17619b26240b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_dsolve/linsolve.py:214: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
            "  warn('spsolve requires A be CSC or CSR matrix format',\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_dsolve/linsolve.py:285: SparseEfficiencyWarning: spsolve is more efficient when sparse b is in the CSC matrix format\n",
            "  warn('spsolve is more efficient when sparse b '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1473):\n",
        "  print(laplacians[1].getrow(i).getcol(i).toarray()[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Htj9XYsqCzSE",
        "outputId": "cafc5951-23cf-42ff-a8d8-eee7e9e29bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3259099\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20390324\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.26123047\n",
            "0.2\n",
            "0.2\n",
            "0.15384616\n",
            "0.16227514\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.20340891\n",
            "0.15384616\n",
            "0.16227515\n",
            "0.15384616\n",
            "0.17265742\n",
            "0.18181819\n",
            "0.21989362\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20097613\n",
            "0.31838745\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20390315\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.2\n",
            "0.2538233\n",
            "0.22222213\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20390323\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.373529\n",
            "0.37352902\n",
            "0.37352896\n",
            "0.32590994\n",
            "0.2406963\n",
            "0.23500086\n",
            "0.2592433\n",
            "0.23500088\n",
            "0.288873\n",
            "0.43702102\n",
            "0.24069634\n",
            "0.23500088\n",
            "0.23500088\n",
            "0.28887293\n",
            "0.37352896\n",
            "0.65924335\n",
            "0.5259099\n",
            "0.37352893\n",
            "0.33618373\n",
            "0.300826\n",
            "0.437021\n",
            "0.288873\n",
            "0.43702105\n",
            "0.2592433\n",
            "0.28887296\n",
            "0.23500088\n",
            "0.25924322\n",
            "0.28887293\n",
            "0.28887293\n",
            "0.28887296\n",
            "0.25924334\n",
            "0.288873\n",
            "0.2888729\n",
            "0.6592433\n",
            "0.23500088\n",
            "0.25924334\n",
            "0.2592433\n",
            "0.32590994\n",
            "0.2406963\n",
            "0.32590997\n",
            "0.2592433\n",
            "0.26981682\n",
            "0.32590994\n",
            "0.23500088\n",
            "0.2888729\n",
            "0.25924334\n",
            "0.24069631\n",
            "0.32590997\n",
            "0.28887293\n",
            "0.32590997\n",
            "0.32590994\n",
            "0.25924328\n",
            "0.29796082\n",
            "0.3259099\n",
            "0.25924334\n",
            "0.32590994\n",
            "0.28887296\n",
            "0.24069628\n",
            "0.52590996\n",
            "0.28887293\n",
            "0.23500088\n",
            "0.2592433\n",
            "0.332266\n",
            "0.2577137\n",
            "0.37352896\n",
            "0.28887296\n",
            "0.28887296\n",
            "0.32590994\n",
            "0.33618304\n",
            "0.25924328\n",
            "0.32591\n",
            "0.37352905\n",
            "0.28887296\n",
            "0.25924325\n",
            "0.32591\n",
            "0.32590997\n",
            "0.35259783\n",
            "0.28887296\n",
            "0.2592433\n",
            "0.37352902\n",
            "0.28887293\n",
            "0.32591\n",
            "0.29796076\n",
            "0.16227514\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.20340966\n",
            "0.15384616\n",
            "0.16227515\n",
            "0.15384616\n",
            "0.17265742\n",
            "0.2857143\n",
            "0.2857143\n",
            "0.2857143\n",
            "0.22874951\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20462503\n",
            "0.18181819\n",
            "0.42949882\n",
            "0.31838748\n",
            "0.22914769\n",
            "0.31838748\n",
            "0.32183868\n",
            "0.28135052\n",
            "0.31838757\n",
            "0.31838754\n",
            "0.28135034\n",
            "0.31838748\n",
            "0.42949888\n",
            "0.2813504\n",
            "0.3218388\n",
            "0.3218388\n",
            "0.22874956\n",
            "0.22874953\n",
            "0.31838754\n",
            "0.22914769\n",
            "0.22874942\n",
            "0.27422544\n",
            "0.28135055\n",
            "0.28135052\n",
            "0.22914769\n",
            "0.22874953\n",
            "0.39720556\n",
            "0.22874948\n",
            "0.262972\n",
            "0.31838757\n",
            "0.3183875\n",
            "0.3183875\n",
            "0.2813505\n",
            "0.22914767\n",
            "0.22914767\n",
            "0.32183874\n",
            "0.2291477\n",
            "0.22914767\n",
            "0.4294984\n",
            "0.22874954\n",
            "0.2857143\n",
            "0.2857143\n",
            "0.2857143\n",
            "0.2857143\n",
            "0.2857143\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.23052368\n",
            "0.19151515\n",
            "0.19151515\n",
            "0.18181819\n",
            "0.25382265\n",
            "0.2222222\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16966793\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.26123047\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.26123017\n",
            "0.2\n",
            "0.2\n",
            "0.33333334\n",
            "0.33333334\n",
            "0.22222222\n",
            "0.29086062\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.21678561\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.18181819\n",
            "0.20390318\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.21187066\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.29086053\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.15384616\n",
            "0.1640704\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.1640704\n",
            "0.15384616\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.29086047\n",
            "0.22222222\n",
            "0.2857143\n",
            "0.2857143\n",
            "0.2857143\n",
            "0.335896\n",
            "0.18181819\n",
            "0.2191776\n",
            "0.18181819\n",
            "0.20009479\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.1926174\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.16666667\n",
            "0.19579934\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.2\n",
            "0.24024366\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.27900806\n",
            "0.24000001\n",
            "0.22222222\n",
            "0.24000001\n",
            "0.16407044\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16407044\n",
            "0.15384616\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.2305231\n",
            "0.19151516\n",
            "0.19151516\n",
            "0.18181819\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.18553944\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.21215598\n",
            "0.18055551\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.25\n",
            "0.25\n",
            "0.20390318\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.18553947\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.33333334\n",
            "0.33333334\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.18553942\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.19351153\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.2857143\n",
            "0.2857143\n",
            "0.3358959\n",
            "0.21917766\n",
            "0.18181819\n",
            "0.20009479\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.1926174\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.23052368\n",
            "0.19151516\n",
            "0.19151516\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16966791\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.28898674\n",
            "0.29481223\n",
            "0.30790693\n",
            "0.25\n",
            "0.31166273\n",
            "0.2857143\n",
            "0.2857143\n",
            "0.21989363\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20097612\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2612305\n",
            "0.2\n",
            "0.2\n",
            "0.24024378\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.4\n",
            "0.19579925\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.24024373\n",
            "0.24024372\n",
            "0.3109749\n",
            "0.19579919\n",
            "0.31097496\n",
            "0.19579934\n",
            "0.19579922\n",
            "0.19579935\n",
            "0.24024373\n",
            "0.43093833\n",
            "0.3743509\n",
            "0.24024373\n",
            "0.24024372\n",
            "0.19579932\n",
            "0.31097493\n",
            "0.19579926\n",
            "0.19579934\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.26123026\n",
            "0.2\n",
            "0.2\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.29086012\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.21678591\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.25\n",
            "0.25\n",
            "0.29875988\n",
            "0.25\n",
            "0.2857143\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.34260255\n",
            "0.39626193\n",
            "0.3587445\n",
            "0.28898677\n",
            "0.28921446\n",
            "0.28921446\n",
            "0.34655568\n",
            "0.28921446\n",
            "0.3639512\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.29086\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.29086003\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.21409048\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.1951026\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20489852\n",
            "0.25\n",
            "0.16407044\n",
            "0.16407046\n",
            "0.16407044\n",
            "0.16407044\n",
            "0.28457475\n",
            "0.28457475\n",
            "0.28457475\n",
            "0.39189562\n",
            "0.16407043\n",
            "0.16407044\n",
            "0.11764706\n",
            "0.16407043\n",
            "0.16227514\n",
            "0.16227514\n",
            "0.2453095\n",
            "0.16227514\n",
            "0.21812348\n",
            "0.16227514\n",
            "0.27337906\n",
            "0.11764706\n",
            "0.16227514\n",
            "0.12851508\n",
            "0.24530958\n",
            "0.22222222\n",
            "0.27900818\n",
            "0.23999998\n",
            "0.22222222\n",
            "0.23999998\n",
            "0.33304703\n",
            "0.21409045\n",
            "0.17908813\n",
            "0.2198936\n",
            "0.21989362\n",
            "0.21989359\n",
            "0.21409044\n",
            "0.1343838\n",
            "0.14449684\n",
            "0.33869195\n",
            "0.21989363\n",
            "0.25607917\n",
            "0.21989363\n",
            "0.33869198\n",
            "0.21917765\n",
            "0.13076153\n",
            "0.14449686\n",
            "0.21917765\n",
            "0.21409048\n",
            "0.15392391\n",
            "0.21409047\n",
            "0.21409045\n",
            "0.17908809\n",
            "0.17908812\n",
            "0.21917765\n",
            "0.25607917\n",
            "0.08533074\n",
            "0.1670321\n",
            "0.17908815\n",
            "0.17908812\n",
            "0.17908806\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16407044\n",
            "0.15384616\n",
            "0.33333334\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.17482509\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.2121563\n",
            "0.18055548\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16966791\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16407046\n",
            "0.15384616\n",
            "0.29086\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.3203312\n",
            "0.31707457\n",
            "0.32385498\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.30462277\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16407044\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16407044\n",
            "0.15384616\n",
            "0.18181819\n",
            "0.19510262\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20489854\n",
            "0.15886147\n",
            "0.15886147\n",
            "0.15828146\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16989614\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.33333334\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.21678585\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.2857143\n",
            "0.33589593\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.21678591\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20097616\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.29085997\n",
            "0.22222222\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20097613\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.20340924\n",
            "0.15384616\n",
            "0.16227515\n",
            "0.15384616\n",
            "0.17265742\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20097612\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.18553941\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.2039032\n",
            "0.20390318\n",
            "0.20390321\n",
            "0.3666861\n",
            "0.29481223\n",
            "0.37145218\n",
            "0.19510262\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20489852\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16966785\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.17482509\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.25\n",
            "0.3368705\n",
            "0.2976943\n",
            "0.25\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.26123035\n",
            "0.2\n",
            "0.2\n",
            "0.26400074\n",
            "0.24376705\n",
            "0.2640007\n",
            "0.1951026\n",
            "0.36073014\n",
            "0.1951026\n",
            "0.19510265\n",
            "0.24376705\n",
            "0.12519193\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.21678592\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.2\n",
            "0.2\n",
            "0.26123032\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.26123035\n",
            "0.2\n",
            "0.2\n",
            "0.25\n",
            "0.25\n",
            "0.33485842\n",
            "0.28457475\n",
            "0.25\n",
            "0.29875988\n",
            "0.25\n",
            "0.29875988\n",
            "0.25\n",
            "0.16666667\n",
            "0.19351158\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.19351155\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.22222222\n",
            "0.29086\n",
            "0.22222222\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.2167859\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.2\n",
            "0.26123032\n",
            "0.2\n",
            "0.2\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16966785\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.23866883\n",
            "0.125\n",
            "0.16402054\n",
            "0.15886147\n",
            "0.15886149\n",
            "0.23866886\n",
            "0.13530496\n",
            "0.15886149\n",
            "0.15886152\n",
            "0.15886149\n",
            "0.2857143\n",
            "0.33922136\n",
            "0.32494274\n",
            "0.18181819\n",
            "0.20097616\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.23866884\n",
            "0.22222222\n",
            "0.2468872\n",
            "0.20097616\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20462501\n",
            "0.18181819\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.21678591\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.33922133\n",
            "0.32494274\n",
            "0.29086\n",
            "0.22222222\n",
            "0.16666667\n",
            "0.21678592\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.20009479\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.1926174\n",
            "0.3368705\n",
            "0.2976943\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20462503\n",
            "0.18181819\n",
            "0.27900815\n",
            "0.24\n",
            "0.22222222\n",
            "0.24\n",
            "0.2\n",
            "0.26123032\n",
            "0.2\n",
            "0.2\n",
            "0.20009479\n",
            "0.20009479\n",
            "0.21238934\n",
            "0.2\n",
            "0.2\n",
            "0.26123038\n",
            "0.2\n",
            "0.2\n",
            "0.18181819\n",
            "0.23052326\n",
            "0.19151516\n",
            "0.19151516\n",
            "0.18181819\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.18553944\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.1640205\n",
            "0.15886149\n",
            "0.15886149\n",
            "0.23866883\n",
            "0.13530494\n",
            "0.15886149\n",
            "0.15886149\n",
            "0.1588615\n",
            "0.25\n",
            "0.25\n",
            "0.18181819\n",
            "0.21187066\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.16666667\n",
            "0.21678591\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.17482509\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.33589593\n",
            "0.18181819\n",
            "0.1926174\n",
            "0.15384616\n",
            "0.17482506\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.18181819\n",
            "0.15384616\n",
            "0.20340925\n",
            "0.15384616\n",
            "0.16227515\n",
            "0.15384616\n",
            "0.17265742\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20462501\n",
            "0.18181819\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.16666667\n",
            "0.1855395\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.26123038\n",
            "0.2\n",
            "0.2\n",
            "0.17482506\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.25\n",
            "0.33485842\n",
            "0.28457475\n",
            "0.23052329\n",
            "0.19151515\n",
            "0.19151515\n",
            "0.18181819\n",
            "0.21187066\n",
            "0.17482507\n",
            "0.21187066\n",
            "0.21187066\n",
            "0.21187066\n",
            "0.17482507\n",
            "0.17482509\n",
            "0.21187066\n",
            "0.1748251\n",
            "0.17482509\n",
            "0.2\n",
            "0.26123038\n",
            "0.2\n",
            "0.2\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.33485842\n",
            "0.28457475\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.21215625\n",
            "0.18055552\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.21215625\n",
            "0.18055558\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20489854\n",
            "0.15828146\n",
            "0.15828146\n",
            "0.144732\n",
            "0.15828146\n",
            "0.15828146\n",
            "0.15828146\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.18181819\n",
            "0.20489858\n",
            "0.20489855\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.21678586\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.2\n",
            "0.2\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.2\n",
            "0.2\n",
            "0.26123038\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.25\n",
            "0.25\n",
            "0.2\n",
            "0.2\n",
            "0.15384616\n",
            "0.16966788\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.24624401\n",
            "0.2453095\n",
            "0.2559159\n",
            "0.22222222\n",
            "0.2\n",
            "0.26123035\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.20340924\n",
            "0.15384616\n",
            "0.16227515\n",
            "0.15384616\n",
            "0.17265742\n",
            "0.25\n",
            "0.22222222\n",
            "0.26123038\n",
            "0.2\n",
            "0.2\n",
            "0.26123032\n",
            "0.2\n",
            "0.2\n",
            "0.21678586\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.29086006\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.26123032\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.15384616\n",
            "0.16989622\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.2\n",
            "0.16989622\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.21215625\n",
            "0.18055554\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.14271182\n",
            "0.21215628\n",
            "0.17234147\n",
            "0.27900812\n",
            "0.20340924\n",
            "0.2538229\n",
            "0.26123038\n",
            "0.40938574\n",
            "0.26123038\n",
            "0.21215627\n",
            "0.26123032\n",
            "0.2908599\n",
            "0.21678586\n",
            "0.17234147\n",
            "0.2167859\n",
            "0.26123038\n",
            "0.21678588\n",
            "0.25382292\n",
            "0.21678594\n",
            "0.25382298\n",
            "0.29086003\n",
            "0.3353864\n",
            "0.25382292\n",
            "0.2612303\n",
            "0.21812348\n",
            "0.21678591\n",
            "0.21678588\n",
            "0.29086006\n",
            "0.26123038\n",
            "0.26123035\n",
            "0.3353864\n",
            "0.39189562\n",
            "0.3368705\n",
            "0.26123038\n",
            "0.26123035\n",
            "0.29086\n",
            "0.40902367\n",
            "0.33538643\n",
            "0.21215627\n",
            "0.21678588\n",
            "0.29086003\n",
            "0.20340925\n",
            "0.29085997\n",
            "0.23474616\n",
            "0.23052327\n",
            "0.29086\n",
            "0.26123038\n",
            "0.25\n",
            "0.18055555\n",
            "0.22222218\n",
            "0.18055551\n",
            "0.2222222\n",
            "0.22222221\n",
            "0.22222222\n",
            "0.18055558\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.23999998\n",
            "0.13333334\n",
            "0.19151515\n",
            "0.22222222\n",
            "0.1926174\n",
            "0.23999998\n",
            "0.16966788\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16227515\n",
            "0.15384616\n",
            "0.17265742\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.20462504\n",
            "0.18181819\n",
            "0.2468872\n",
            "0.29875988\n",
            "0.16966787\n",
            "0.16966785\n",
            "0.16966787\n",
            "0.20462501\n",
            "0.18181819\n",
            "0.3927833\n",
            "0.16989622\n",
            "0.16989619\n",
            "0.16989623\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.2976943\n",
            "0.20462501\n",
            "0.2\n",
            "0.22222222\n",
            "0.2\n",
            "0.25\n",
            "0.25\n",
            "0.2\n",
            "0.2\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.27337906\n",
            "0.28553873\n",
            "0.24624401\n",
            "0.16666667\n",
            "0.2\n",
            "0.25\n",
            "0.19351155\n",
            "0.30790693\n",
            "0.19351156\n",
            "0.19351155\n",
            "0.30462268\n",
            "0.30462274\n",
            "0.30462268\n",
            "0.19351156\n",
            "0.30462265\n",
            "0.19351156\n",
            "0.19351153\n",
            "0.3897312\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.19151515\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.2\n",
            "0.2\n",
            "0.2\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.15384616\n",
            "0.16407043\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.18181819\n",
            "0.18181819\n",
            "0.2857143\n",
            "0.2\n",
            "0.2\n",
            "0.16666667\n",
            "0.2\n",
            "0.3116628\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.16407044\n",
            "0.15384616\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.22222222\n",
            "0.18181819\n",
            "0.15384616\n",
            "0.25\n",
            "0.2973233\n",
            "0.25\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.2\n",
            "0.2\n",
            "0.16227515\n",
            "0.12851508\n",
            "0.24530949\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16666667\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.2\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.18553945\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.25\n",
            "0.2973233\n",
            "0.25\n",
            "0.16666667\n",
            "0.16666667\n",
            "0.15384616\n",
            "0.16407043\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.15384616\n",
            "0.16666667\n",
            "0.2\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.22222222\n",
            "0.2973233\n",
            "0.18553945\n",
            "0.18553947\n",
            "0.16666667\n",
            "0.25\n",
            "0.16666667\n",
            "0.17265742\n",
            "0.22222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(laplacians[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpxiPoDDS7wJ",
        "outputId": "4cd99e09-b54d-48ac-c5f7-47c8a620097a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t0.3259099\n",
            "  (9, 0)\t-0.00012907143\n",
            "  (17, 0)\t-8.5398024e-05\n",
            "  (21, 0)\t-0.000879516\n",
            "  (25, 0)\t0.0006843986\n",
            "  (27, 0)\t-0.0008795162\n",
            "  (29, 0)\t-0.0005503578\n",
            "  (31, 0)\t-0.00036336784\n",
            "  (37, 0)\t-0.0012107567\n",
            "  (38, 0)\t-1.1055643e-06\n",
            "  (46, 0)\t-0.00012907141\n",
            "  (51, 0)\t-8.5398024e-05\n",
            "  (59, 0)\t-0.00012907143\n",
            "  (63, 0)\t0.00742336\n",
            "  (64, 0)\t0.0074233664\n",
            "  (65, 0)\t0.0074233674\n",
            "  (66, 0)\t0.0074233618\n",
            "  (67, 0)\t0.008366763\n",
            "  (68, 0)\t0.0074233683\n",
            "  (69, 0)\t0.0074233674\n",
            "  (70, 0)\t0.0074233664\n",
            "  (71, 0)\t0.0074233594\n",
            "  (72, 0)\t0.007423363\n",
            "  (73, 0)\t0.008366767\n",
            "  (74, 0)\t0.007423368\n",
            "  :\t:\n",
            "  (1364, 1473)\t-0.0031911202\n",
            "  (1365, 1473)\t-0.0031911198\n",
            "  (1366, 1473)\t-0.00319112\n",
            "  (1367, 1473)\t-0.0031911207\n",
            "  (1368, 1473)\t-0.00319112\n",
            "  (1369, 1473)\t-0.0031911202\n",
            "  (1370, 1473)\t-0.0031911202\n",
            "  (1371, 1473)\t-0.0031911198\n",
            "  (1372, 1473)\t-0.0031911193\n",
            "  (1373, 1473)\t0.03946359\n",
            "  (1397, 1473)\t9.456306e-05\n",
            "  (1408, 1473)\t0.033879135\n",
            "  (1411, 1473)\t9.456306e-05\n",
            "  (1421, 1473)\t-0.0007806291\n",
            "  (1431, 1473)\t-0.0016490674\n",
            "  (1432, 1473)\t-0.010737206\n",
            "  (1433, 1473)\t0.0020414244\n",
            "  (1447, 1473)\t0.00069389254\n",
            "  (1451, 1473)\t-0.0007806291\n",
            "  (1456, 1473)\t-9.4563045e-05\n",
            "  (1465, 1473)\t0.0007806291\n",
            "  (1466, 1473)\t-0.00069389254\n",
            "  (1467, 1473)\t-0.00069389254\n",
            "  (1471, 1473)\t0.014441285\n",
            "  (1473, 1473)\t0.25591594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnzQGe23BJ-8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "outputId": "116fd1df-4a45-4b85-9770-7dbe72c10b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/simplicial_neural_networks/data/s2_3_collaboration_complex\n",
            "/content/simplicial_neural_networks\n",
            "Parameter counts:\n",
            "150\n",
            "30\n",
            "4500\n",
            "30\n",
            "150\n",
            "1\n",
            "150\n",
            "30\n",
            "4500\n",
            "30\n",
            "150\n",
            "1\n",
            "150\n",
            "30\n",
            "4500\n",
            "30\n",
            "150\n",
            "1\n",
            "Total number of parameters: 14583\n",
            "/content/simplicial_neural_networks/data/s2_3_collaboration_complex\n",
            "[0.6988636363636364, 0.6994572591587517, 0.6998477929984779]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-2d897450f28a>\u001b[0m in \u001b[0;36m<cell line: 123>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-2d897450f28a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mlosslogf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mname_networks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'C0_1,C0_2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C0_3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C1_1,C1_2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C1_3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C2_1,C2_2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C2_3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'losslogf' is not defined"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    torch.manual_seed(1337)\n",
        "    np.random.seed(1337)\n",
        "\n",
        "\n",
        "    prefix = \"./data/s2_3_collaboration_complex\" ##input\n",
        "\n",
        "    logdir = \"./experiments/output\" ##output\n",
        "    starting_node=\"150250\"\n",
        "    percentage_missing_values=\"30\"\n",
        "    cuda = False\n",
        "\n",
        "    topdim = 2\n",
        "    %cd '/content/simplicial_neural_networks/data/s2_3_collaboration_complex'\n",
        "\n",
        "    laplacians = np.load('{}_laplacians.npy'.format(starting_node),allow_pickle=True)\n",
        "    boundaries = np.load('{}_boundaries.npy'.format(starting_node),allow_pickle=True)\n",
        "    %cd /content/simplicial_neural_networks\n",
        "\n",
        "\n",
        "    Ls =[coo2tensor(chebyshev.normalize(laplacians[i],half_interval=True)) for i in range(topdim+1)] #####scnn.chebyshev.normalize ?\n",
        "    Ds=[coo2tensor(boundaries[i].transpose()) for i in range(topdim+1)]\n",
        "    adDs=[coo2tensor(boundaries[i]) for i in range(topdim+1)]\n",
        "\n",
        "\n",
        "    network = MySCNN(colors = 1)\n",
        "\n",
        "\n",
        "    learning_rate = 0.001\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
        "    criterion = nn.L1Loss(reduction=\"sum\")\n",
        "    #criterion = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "    batch_size = 1\n",
        "\n",
        "    num_params = 0\n",
        "    print(\"Parameter counts:\")\n",
        "    for param in network.parameters():\n",
        "        p = np.array(param.shape, dtype=int).prod()\n",
        "        print(p)\n",
        "        num_params += p\n",
        "    print(\"Total number of parameters: %d\" %(num_params))\n",
        "\n",
        "    %cd /content/simplicial_neural_networks/data/s2_3_collaboration_complex\n",
        "    masks_all_deg = np.load('{}_percentage_{}_known_values.npy'.format(starting_node,percentage_missing_values),allow_pickle=True) ## positive mask= indices that we keep ##1 mask #entries 0 degree\n",
        "    masks=[list(masks_all_deg[i].values()) for i in range(len(masks_all_deg))]\n",
        "    # losslogf = open(\"%s/loss.txt\" %(logdir), \"w\")\n",
        "\n",
        "    cochain_target_alldegs = []\n",
        "    signal = np.load('{}_cochains.npy'.format(starting_node),allow_pickle=True)\n",
        "    raw_data=[list(signal[i].values()) for i in range(len(signal))]\n",
        "    for d in range(0, topdim+1):\n",
        "        cochain_target = torch.zeros((batch_size, 1, len(raw_data[d])), dtype=torch.float, requires_grad = False)\n",
        "        for i in range(0, batch_size):\n",
        "            cochain_target[i, 0, :] = torch.tensor(raw_data[d], dtype=torch.float, requires_grad = False)\n",
        "\n",
        "        cochain_target_alldegs.append(cochain_target)\n",
        "\n",
        "    cochain_input_alldegs = []\n",
        "    signal = np.load('{}_percentage_{}_input_damaged.npy'.format(starting_node,percentage_missing_values),allow_pickle=True)\n",
        "    raw_data=[list(signal[i].values()) for i in range(len(signal))]\n",
        "    for d in range(0, topdim+1):\n",
        "\n",
        "        cochain_input = torch.zeros((batch_size, 1, len(raw_data[d])), dtype=torch.float, requires_grad = False)\n",
        "\n",
        "        for i in range(0, batch_size):\n",
        "            cochain_input[i, 0, :] = torch.tensor(raw_data[d], dtype=torch.float, requires_grad = False)\n",
        "\n",
        "        cochain_input_alldegs.append(cochain_input)\n",
        "\n",
        "    #cochain_target_alldegs[0] = torch.zeros_like(cochain_target_alldegs[0])\n",
        "    #cochain_target_alldegs[2] = torch.zeros_like(cochain_target_alldegs[2])\n",
        "\n",
        "    #cochain_input_alldegs[0] = torch.zeros_like(cochain_input_alldegs[0])\n",
        "    #cochain_input_alldegs[2] = torch.zeros_like(cochain_input_alldegs[2])\n",
        "\n",
        "    print([float(len(masks[d]))/float(len(cochain_target_alldegs[d][0,0,:])) for d in range(0,2+1)])\n",
        "\n",
        "    for i in range(0, 1000):\n",
        "        xs = [cochain_input.clone() for cochain_input in cochain_input_alldegs]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        ys = network(Ls, Ds, adDs, xs)\n",
        "\n",
        "        loss = torch.FloatTensor([0.0])\n",
        "        for b in range(0, batch_size):\n",
        "            for d in range(0, topdim+1):\n",
        "                loss += criterion(ys[d][b, 0, masks[d]], cochain_target_alldegs[d][b, 0, masks[d]])\n",
        "\n",
        "        detached_ys = [ys[d].detach() for d in range(0, topdim+1)]\n",
        "\n",
        "        # if np.mod(i, 10) == 0:\n",
        "        #     for d in range(0,topdim+1):\n",
        "        #         np.savetxt(\"%s/output_%d_%d.txt\" %(logdir, i, d), detached_ys[d][0,0,:])\n",
        "\n",
        "        for d in range(0, topdim+1):\n",
        "            predictionlogf = open(\"%s/prediction_%d_%d.txt\" %(logdir, i, d), \"w\")\n",
        "            actuallogf = open(\"%s/actual_%d_%d.txt\" %(logdir, i, d), \"w\")\n",
        "\n",
        "            for b in range(0, batch_size):\n",
        "                for y in detached_ys[d][b, 0, masks[d]]:\n",
        "                    predictionlogf.write(\"%f \" %(y))\n",
        "                predictionlogf.write(\"\\n\")\n",
        "                for x in cochain_target_alldegs[d][b, 0, masks[d]]:\n",
        "                    actuallogf.write(\"%f \" %(x))\n",
        "                actuallogf.write(\"\\n\")\n",
        "            predictionlogf.close()\n",
        "            actuallogf.close()\n",
        "\n",
        "\n",
        "        losslogf.write(\"%d %f\\n\" %(i, loss.item()))\n",
        "        losslogf.flush()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # losslogf.close()\n",
        "\n",
        "    name_networks=['C0_1,C0_2','C0_3','C1_1,C1_2','C1_3', 'C2_1,C2_2','C2_3']\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_networks=['C0_1,C0_2','C0_3','C1_1,C1_2','C1_3', 'C2_1,C2_2','C2_3']\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "6nma7_OxIpVT",
        "outputId": "30be26cb-92f7-4519-810b-6344e1552334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/simplicial_neural_networks/data/s2_3_collaboration_complex\n",
            "/content/simplicial_neural_networks\n",
            "Parameter counts:\n",
            "150\n",
            "30\n",
            "4500\n",
            "30\n",
            "150\n",
            "1\n",
            "150\n",
            "30\n",
            "4500\n",
            "30\n",
            "150\n",
            "1\n",
            "150\n",
            "30\n",
            "4500\n",
            "30\n",
            "150\n",
            "1\n",
            "Total number of parameters: 14583\n",
            "/content/simplicial_neural_networks/data/s2_3_collaboration_complex\n",
            "[0.6988636363636364, 0.6994572591587517, 0.6998477929984779]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-0f6aaf7a9092>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-2d897450f28a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-0ce57aea32f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Ls, Ds, adDs, xs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mout0_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC0_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout0_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#+ self.D10_2(nn.LeakyReLU()(out1_1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mout1_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC1_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#+ self.D01_2(nn.LeakyReLU()(out0_1)) + self.D21_2(nn.LeakyReLU()(out2_1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mout2_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC2_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#+ self.D12_2(nn.LeakyReLU()(out1_1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mout0_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC0_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout0_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#+ self.D10_3(nn.LeakyReLU()(out1_2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/simplicial_neural_networks/scnn/scnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, L, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_in\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchebyshev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bimk,oik->bom\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/simplicial_neural_networks/scnn/chebyshev.py\u001b[0m in \u001b[0;36massemble\u001b[0;34m(K, L, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mX23\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX23\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mX23\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX23\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX23\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}