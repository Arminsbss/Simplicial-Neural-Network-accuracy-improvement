{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arminsbss/Simplicial-Neural-Network/blob/main/accuracy%20improvement2/finding_optimal_laplacians_2__coefficient_using_lrelu_0_1_epoch4000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGnOA_G72-Wm",
        "outputId": "c45c48b8-b781-4181-a786-2ad022bc5071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'simplicial_neural_networks'...\n",
            "remote: Enumerating objects: 339, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 339 (delta 50), reused 48 (delta 48), pack-reused 280\u001b[K\n",
            "Receiving objects: 100% (339/339), 2.42 MiB | 11.12 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Arminsbss/Simplicial-Neural-Network-accuracy-improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffVIBHwdMFX9",
        "outputId": "c8c6a6ce-c71c-4486-c757-dc9ff2bde908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/simplicial_neural_networks/data/s2_3_collaboration_complex\n",
            "75 .  -0.1->-0.01, 76 .  -0.1->-0.01, 81 .  -0.1->-0.01, 82 .  -0.1->-0.01, 132 .  -0.1->-0.01, 300 .  -0.1->-0.01, 301 .  -0.1->-0.01, \n",
            "302 .  -0.1->-0.01, 303 .  -0.1->-0.01, 304 .  -0.1->-0.01, 309 .  -0.1->-0.01, 310 .  -0.1->-0.01, 311 .  -0.1->-0.01, 312 .  -0.1->-0.01, \n",
            "313 .  -0.1->-0.01, 314 .  -0.1->-0.01, 320 .  -0.1->-0.01, 321 .  -0.1->-0.01, 322 .  -0.1->-0.01, 323 .  -0.1->-0.01, 324 .  -0.1->-0.01, \n",
            "328 .  -0.1->-0.01, 329 .  -0.1->-0.01, 330 .  -0.1->-0.01, 452 .  -0.1->-0.01, 467 .  -0.1->-0.01, 468 .  -0.1->-0.01, 473 .  -0.1->-0.01, \n",
            "474 .  -0.1->-0.01, 667 .  -0.1->-0.01, 673 .  -0.1->-0.01, 909 .  -0.1->-0.01, 910 .  -0.1->-0.01, 911 .  -0.1->-0.01, 912 .  -0.1->-0.01, \n",
            "973 .  -0.1->-0.01, 974 .  -0.1->-0.01, 975 .  -0.1->-0.01, 976 .  -0.1->-0.01, 977 .  -0.1->-0.01, 981 .  -0.1->-0.01, 982 .  -0.1->-0.01, \n",
            "983 .  -0.1->-0.01, 1112 .  -0.1->-0.01, 1113 .  -0.1->-0.01, 1114 .  -0.1->-0.01, 1115 .  -0.1->-0.01, 1184 .  -0.1->-0.01, 1190 .  -0.1->-0.01, \n",
            "1396 .  -0.1->-0.01, 1397 .  -0.1->-0.01, 1398 .  -0.1->-0.01, 1399 .  -0.1->-0.01, 1400 .  -0.1->-0.01, 1404 .  -0.1->-0.01, 1405 .  -0.1->-0.01, \n",
            "1406 .  -0.1->-0.01, 1435 .  -0.1->-0.01, 1441 .  -0.1->-0.01, 1526 .  -0.1->-0.01, 1768 .  -0.1->-0.01, 1778 .  -0.1->-0.01, 1779 .  -0.1->-0.01, \n",
            "1780 .  -0.1->-0.01, 1781 .  -0.1->-0.01, 1782 .  -0.1->-0.01, 1783 .  -0.1->-0.01, 1784 .  -0.1->-0.01, 1785 .  -0.1->-0.01, 1786 .  -0.1->-0.01, \n",
            "1787 .  -0.1->-0.01, 1788 .  -0.1->-0.01, 1809 .  -0.1->-0.01, 1810 .  -0.1->-0.01, 1811 .  -0.1->-0.01, 1812 .  -0.1->-0.01, 1813 .  -0.1->-0.01, \n",
            "1862 .  -0.1->-0.01, 1863 .  -0.1->-0.01, 1870 .  -0.1->-0.01, 1874 .  -0.1->-0.01, 1875 .  -0.1->-0.01, 1882 .  -0.1->-0.01, 1885 .  -0.1->-0.01, \n",
            "1886 .  -0.1->-0.01, 1893 .  -0.1->-0.01, 1895 .  -0.1->-0.01, 1896 .  -0.1->-0.01, 1903 .  -0.1->-0.01, 1917 .  -0.1->-0.01, 1918 .  -0.1->-0.01, \n",
            "1919 .  -0.1->-0.01, 1920 .  -0.1->-0.01, 1927 .  -0.1->-0.01, 1930 .  -0.1->-0.01, 1933 .  -0.1->-0.01, 1936 .  -0.1->-0.01, 1941 .  -0.1->-0.01, \n",
            "1944 .  -0.1->-0.01, 1947 .  -0.1->-0.01, 1950 .  -0.1->-0.01, 1953 .  -0.1->-0.01, 1954 .  -0.1->-0.01, 1960 .  -0.1->-0.01, 1962 .  -0.1->-0.01, \n",
            "1963 .  -0.1->-0.01, 1974 .  -0.1->-0.01, 1977 .  -0.1->-0.01, 1980 .  -0.1->-0.01, 1983 .  -0.1->-0.01, 1986 .  -0.1->-0.01, 1989 .  -0.1->-0.01, \n",
            "1992 .  -0.1->-0.01, 1995 .  -0.1->-0.01, 1997 .  -0.1->-0.01, 2000 .  -0.1->-0.01, 2003 .  -0.1->-0.01, 2006 .  -0.1->-0.01, 2008 .  -0.1->-0.01, \n",
            "2009 .  -0.1->-0.01, 2015 .  -0.1->-0.01, 2017 .  -0.1->-0.01, 2018 .  -0.1->-0.01, 2027 .  -0.1->-0.01, 2028 .  -0.1->-0.01, 2034 .  -0.1->-0.01, \n",
            "2035 .  -0.1->-0.01, 2036 .  -0.1->-0.01, 2037 .  -0.1->-0.01, 2038 .  -0.1->-0.01, 2042 .  -0.1->-0.01, 2045 .  -0.1->-0.01, 2048 .  -0.1->-0.01, \n",
            "2049 .  -0.1->-0.01, 2050 .  -0.1->-0.01, 2051 .  -0.1->-0.01, 2052 .  -0.1->-0.01, 2053 .  -0.1->-0.01, 2084 .  -0.1->-0.01, 2087 .  -0.1->-0.01, \n",
            "2088 .  -0.1->-0.01, 2089 .  -0.1->-0.01, 2090 .  -0.1->-0.01, 2091 .  -0.1->-0.01, 2092 .  -0.1->-0.01, 2093 .  -0.1->-0.01, 2098 .  -0.1->-0.01, \n",
            "2099 .  -0.1->-0.01, 2100 .  -0.1->-0.01, 2101 .  -0.1->-0.01, 2102 .  -0.1->-0.01, 2104 .  -0.1->-0.01, 2119 .  -0.1->-0.01, 2120 .  -0.1->-0.01, \n",
            "2121 .  -0.1->-0.01, 2122 .  -0.1->-0.01, 2126 .  -0.1->-0.01, 2127 .  -0.1->-0.01, 2128 .  -0.1->-0.01, 2129 .  -0.1->-0.01, 2130 .  -0.1->-0.01, \n",
            "2146 .  -0.1->-0.01, 2147 .  -0.1->-0.01, 2148 .  -0.1->-0.01, 2149 .  -0.1->-0.01, 2152 .  -0.1->-0.01, 2153 .  -0.1->-0.01, 2157 .  -0.1->-0.01, \n",
            "2164 .  -0.1->-0.01, 2176 .  -0.1->-0.01, 2185 .  -0.1->-0.01, 2186 .  -0.1->-0.01, 2187 .  -0.1->-0.01, 2189 .  -0.1->-0.01, 2190 .  -0.1->-0.01, \n",
            "2191 .  -0.1->-0.01, 2196 .  -0.1->-0.01, 2197 .  -0.1->-0.01, 2198 .  -0.1->-0.01, 2200 .  -0.1->-0.01, 2201 .  -0.1->-0.01, 2202 .  -0.1->-0.01, \n",
            "2206 .  -0.1->-0.01, 2207 .  -0.1->-0.01, 2208 .  -0.1->-0.01, 2210 .  -0.1->-0.01, 2211 .  -0.1->-0.01, 2212 .  -0.1->-0.01, 2215 .  -0.1->-0.01, \n",
            "2216 .  -0.1->-0.01, 2217 .  -0.1->-0.01, 2218 .  -0.1->-0.01, 2219 .  -0.1->-0.01, 2235 .  -0.1->-0.01, 2236 .  -0.1->-0.01, 2237 .  -0.1->-0.01, \n",
            "2238 .  -0.1->-0.01, 2240 .  -0.1->-0.01, 2241 .  -0.1->-0.01, 2242 .  -0.1->-0.01, 2249 .  -0.1->-0.01, 2250 .  -0.1->-0.01, 2251 .  -0.1->-0.01, \n",
            "2252 .  -0.1->-0.01, 2253 .  -0.1->-0.01, 2254 .  -0.1->-0.01, 2256 .  -0.1->-0.01, 2257 .  -0.1->-0.01, 2258 .  -0.1->-0.01, 2265 .  -0.1->-0.01, \n",
            "2266 .  -0.1->-0.01, 2267 .  -0.1->-0.01, 2274 .  -0.1->-0.01, 2275 .  -0.1->-0.01, 2276 .  -0.1->-0.01, 2277 .  -0.1->-0.01, 2278 .  -0.1->-0.01, \n",
            "2279 .  -0.1->-0.01, 2280 .  -0.1->-0.01, 2288 .  -0.1->-0.01, 2289 .  -0.1->-0.01, 2301 .  -0.1->-0.01, 2302 .  -0.1->-0.01, 2303 .  -0.1->-0.01, \n",
            "2304 .  -0.1->-0.01, 2305 .  -0.1->-0.01, 2306 .  -0.1->-0.01, 2313 .  -0.1->-0.01, 2321 .  -0.1->-0.01, 2322 .  -0.1->-0.01, 2323 .  -0.1->-0.01, \n",
            "2335 .  -0.1->-0.01, 2336 .  -0.1->-0.01, 2337 .  -0.1->-0.01, 2338 .  -0.1->-0.01, 2339 .  -0.1->-0.01, 2348 .  -0.1->-0.01, 2354 .  -0.1->-0.01, \n",
            "2358 .  -0.1->-0.01, 2362 .  -0.1->-0.01, 2371 .  -0.1->-0.01, 2380 .  -0.1->-0.01, 2381 .  -0.1->-0.01, 2385 .  -0.1->-0.01, 2389 .  -0.1->-0.01, \n",
            "2395 .  -0.1->-0.01, 2411 .  -0.1->-0.01, 2412 .  -0.1->-0.01, 2413 .  -0.1->-0.01, 2415 .  -0.1->-0.01, 2416 .  -0.1->-0.01, 2417 .  -0.1->-0.01, \n",
            "2424 .  -0.1->-0.01, 2425 .  -0.1->-0.01, 2426 .  -0.1->-0.01, 2428 .  -0.1->-0.01, 2429 .  -0.1->-0.01, 2430 .  -0.1->-0.01, 2433 .  -0.1->-0.01, \n",
            "2434 .  -0.1->-0.01, 2435 .  -0.1->-0.01, 2436 .  -0.1->-0.01, 2437 .  -0.1->-0.01, 2438 .  -0.1->-0.01, 2441 .  -0.1->-0.01, 2454 .  -0.1->-0.01, \n",
            "2455 .  -0.1->-0.01, 2456 .  -0.1->-0.01, 2457 .  -0.1->-0.01, 2459 .  -0.1->-0.01, 2460 .  -0.1->-0.01, 2461 .  -0.1->-0.01, 2463 .  -0.1->-0.01, \n",
            "2464 .  -0.1->-0.01, 2465 .  -0.1->-0.01, 2467 .  -0.1->-0.01, 2483 .  -0.1->-0.01, 2484 .  -0.1->-0.01, 2485 .  -0.1->-0.01, 2487 .  -0.1->-0.01, \n",
            "2488 .  -0.1->-0.01, 2489 .  -0.1->-0.01, 2495 .  -0.1->-0.01, 2496 .  -0.1->-0.01, 2510 .  -0.1->-0.01, 2511 .  -0.1->-0.01, 2512 .  -0.1->-0.01, \n",
            "2513 .  -0.1->-0.01, 2514 .  -0.1->-0.01, 2518 .  -0.1->-0.01, 2519 .  -0.1->-0.01, 2520 .  -0.1->-0.01, 2521 .  -0.1->-0.01, 2522 .  -0.1->-0.01, \n",
            "2526 .  -0.1->-0.01, 2527 .  -0.1->-0.01, 2528 .  -0.1->-0.01, 2529 .  -0.1->-0.01, 2530 .  -0.1->-0.01, 2535 .  -0.1->-0.01, 2536 .  -0.1->-0.01, \n",
            "2541 .  -0.1->-0.01, 2542 .  -0.1->-0.01, 2545 .  -0.1->-0.01, 2546 .  -0.1->-0.01, 2549 .  -0.1->-0.01, 2558 .  -0.1->-0.01, 2567 .  -0.1->-0.01, \n",
            "2568 .  -0.1->-0.01, 2569 .  -0.1->-0.01, 2572 .  -0.1->-0.01, 2573 .  -0.1->-0.01, 2576 .  -0.1->-0.01, 2577 .  -0.1->-0.01, 2599 .  -0.1->-0.01, \n",
            "2600 .  -0.1->-0.01, 2601 .  -0.1->-0.01, 2602 .  -0.1->-0.01, 2603 .  -0.1->-0.01, 2604 .  -0.1->-0.01, 2606 .  -0.1->-0.01, 2607 .  -0.1->-0.01, \n",
            "2610 .  -0.1->-0.01, 2620 .  -0.1->-0.01, 2621 .  -0.1->-0.01, 2622 .  -0.1->-0.01, 2624 .  -0.1->-0.01, 2625 .  -0.1->-0.01, 2627 .  -0.1->-0.01, \n",
            "2628 .  -0.1->-0.01, 2643 .  -0.1->-0.01, 2644 .  -0.1->-0.01, 2645 .  -0.1->-0.01, 2646 .  -0.1->-0.01, 2659 .  -0.1->-0.01, 2660 .  -0.1->-0.01, \n",
            "2661 .  -0.1->-0.01, 2662 .  -0.1->-0.01, 2665 .  -0.1->-0.01, 2677 .  -0.1->-0.01, 2682 .  -0.1->-0.01, 2694 .  -0.1->-0.01, 2703 .  -0.1->-0.01, \n",
            "2716 .  -0.1->-0.01, 2717 .  -0.1->-0.01, 2718 .  -0.1->-0.01, 2719 .  -0.1->-0.01, 2721 .  -0.1->-0.01, 2722 .  -0.1->-0.01, 2723 .  -0.1->-0.01, \n",
            "2724 .  -0.1->-0.01, 2725 .  -0.1->-0.01, 2726 .  -0.1->-0.01, 2727 .  -0.1->-0.01, 2728 .  -0.1->-0.01, 2729 .  -0.1->-0.01, 2730 .  -0.1->-0.01, \n",
            "2731 .  -0.1->-0.01, 2732 .  -0.1->-0.01, 2733 .  -0.1->-0.01, 2734 .  -0.1->-0.01, 2735 .  -0.1->-0.01, 2740 .  -0.1->-0.01, 2741 .  -0.1->-0.01, \n",
            "2742 .  -0.1->-0.01, 2748 .  -0.1->-0.01, 2749 .  -0.1->-0.01, 2750 .  -0.1->-0.01, 2751 .  -0.1->-0.01, 2752 .  -0.1->-0.01, 2753 .  -0.1->-0.01, \n",
            "2754 .  -0.1->-0.01, 2755 .  -0.1->-0.01, 2756 .  -0.1->-0.01, 2757 .  -0.1->-0.01, 2758 .  -0.1->-0.01, 2762 .  -0.1->-0.01, 2763 .  -0.1->-0.01, \n",
            "2764 .  -0.1->-0.01, 2765 .  -0.1->-0.01, 2766 .  -0.1->-0.01, 2767 .  -0.1->-0.01, 2768 .  -0.1->-0.01, 2769 .  -0.1->-0.01, 2770 .  -0.1->-0.01, \n",
            "2771 .  -0.1->-0.01, 2772 .  -0.1->-0.01, 2776 .  -0.1->-0.01, 2777 .  -0.1->-0.01, 2778 .  -0.1->-0.01, 2779 .  -0.1->-0.01, 2780 .  -0.1->-0.01, \n",
            "2781 .  -0.1->-0.01, 2782 .  -0.1->-0.01, 2783 .  -0.1->-0.01, 2784 .  -0.1->-0.01, 2785 .  -0.1->-0.01, 2786 .  -0.1->-0.01, 2943 .  -0.1->-0.01, \n",
            "2944 .  -0.1->-0.01, 2945 .  -0.1->-0.01, 2946 .  -0.1->-0.01, 2947 .  -0.1->-0.01, 2948 .  -0.1->-0.01, 2949 .  -0.1->-0.01, 2954 .  -0.1->-0.01, \n",
            "2955 .  -0.1->-0.01, 3001 .  -0.1->-0.01, 3002 .  -0.1->-0.01, 3003 .  -0.1->-0.01, 3004 .  -0.1->-0.01, 3005 .  -0.1->-0.01, 3014 .  -0.1->-0.01, \n",
            "3021 .  -0.1->-0.01, 3022 .  -0.1->-0.01, 3032 .  -0.1->-0.01, 3245 .  -0.1->-0.01, 3246 .  -0.1->-0.01, 3247 .  -0.1->-0.01, 3248 .  -0.1->-0.01, \n",
            "3249 .  -0.1->-0.01, 3358 .  -0.1->-0.01, 3359 .  -0.1->-0.01, 3360 .  -0.1->-0.01, 3361 .  -0.1->-0.01, 3367 .  -0.1->-0.01, 3368 .  -0.1->-0.01, \n",
            "3369 .  -0.1->-0.01, 3376 .  -0.1->-0.01, 3377 .  -0.1->-0.01, 3378 .  -0.1->-0.01, 3379 .  -0.1->-0.01, 3386 .  -0.1->-0.01, 3387 .  -0.1->-0.01, \n",
            "3388 .  -0.1->-0.01, 3389 .  -0.1->-0.01, 3395 .  -0.1->-0.01, 3396 .  -0.1->-0.01, 3397 .  -0.1->-0.01, 3404 .  -0.1->-0.01, 3411 .  -0.1->-0.01, \n",
            "3412 .  -0.1->-0.01, 3413 .  -0.1->-0.01, 3424 .  -0.1->-0.01, 3425 .  -0.1->-0.01, 3431 .  -0.1->-0.01, 3438 .  -0.1->-0.01, 3445 .  -0.1->-0.01, \n",
            "3446 .  -0.1->-0.01, 3447 .  -0.1->-0.01, 3448 .  -0.1->-0.01, 3610 .  -0.1->-0.01, 3611 .  -0.1->-0.01, 3612 .  -0.1->-0.01, 3613 .  -0.1->-0.01, \n",
            "3614 .  -0.1->-0.01, 3615 .  -0.1->-0.01, 3616 .  -0.1->-0.01, 3619 .  -0.1->-0.01, 3623 .  -0.1->-0.01, 3624 .  -0.1->-0.01, 3625 .  -0.1->-0.01, \n",
            "3627 .  -0.1->-0.01, 3631 .  -0.1->-0.01, 3635 .  -0.1->-0.01, 3648 .  -0.1->-0.01, 3652 .  -0.1->-0.01, 3656 .  -0.1->-0.01, 3662 .  -0.1->-0.01, \n",
            "3672 .  -0.1->-0.01, 3679 .  -0.1->-0.01, 3686 .  -0.1->-0.01, 3690 .  -0.1->-0.01, 3691 .  -0.1->-0.01, 3692 .  -0.1->-0.01, 3694 .  -0.1->-0.01, \n",
            "3695 .  -0.1->-0.01, 3698 .  -0.1->-0.01, 3699 .  -0.1->-0.01, 3702 .  -0.1->-0.01, 3703 .  -0.1->-0.01, 3725 .  -0.1->-0.01, 3726 .  -0.1->-0.01, \n",
            "3727 .  -0.1->-0.01, 3882 .  -0.1->-0.01, 3888 .  -0.1->-0.01, 3907 .  -0.1->-0.01, 3908 .  -0.1->-0.01, 3917 .  -0.1->-0.01, 3918 .  -0.1->-0.01, \n",
            "3924 .  -0.1->-0.01, 3925 .  -0.1->-0.01, 3931 .  -0.1->-0.01, 3935 .  -0.1->-0.01, 3936 .  -0.1->-0.01, 3937 .  -0.1->-0.01, 3940 .  -0.1->-0.01, \n",
            "3944 .  -0.1->-0.01, 3948 .  -0.1->-0.01, 4102 .  -0.1->-0.01, 4108 .  -0.1->-0.01, 4194 .  -0.1->-0.01, 4195 .  -0.1->-0.01, 4200 .  -0.1->-0.01, \n",
            "4201 .  -0.1->-0.01, 4296 .  -0.1->-0.01, 4465 .  -0.1->-0.01, 4466 .  -0.1->-0.01, 4467 .  -0.1->-0.01, 4468 .  -0.1->-0.01, 4469 .  -0.1->-0.01, \n",
            "4470 .  -0.1->-0.01, 4490 .  -0.1->-0.01, 4536 .  -0.1->-0.01, 4537 .  -0.1->-0.01, 4538 .  -0.1->-0.01, 4539 .  -0.1->-0.01, 4540 .  -0.1->-0.01, \n",
            "4541 .  -0.1->-0.01, 4545 .  -0.1->-0.01, 4595 .  -0.1->-0.01, 4669 .  -0.1->-0.01, 4670 .  -0.1->-0.01, 4671 .  -0.1->-0.01, 4672 .  -0.1->-0.01, \n",
            "4680 .  -0.1->-0.01, /content/simplicial_neural_networks/data/s2_3_collaboration_complex\n",
            "/content/simplicial_neural_networks\n",
            "Parameter counts:\n",
            "150\n",
            "30\n",
            "4500\n",
            "30\n",
            "150\n",
            "1\n",
            "150\n",
            "30\n",
            "4500\n",
            "30\n",
            "150\n",
            "1\n",
            "150\n",
            "30\n",
            "4500\n",
            "30\n",
            "150\n",
            "1\n",
            "Total number of parameters: 14583\n",
            "[0.6988636363636364, 0.6994572591587517, 0.6998477929984779]\n",
            "/content/simplicial_neural_networks/experiments/output\n",
            "We set this random number:  0.47350807102581727\n",
            "Accuracy for 0-laplacian: 0.6809684959349593\n",
            "Accuracy for 1-laplacian: 0.7060732298739089\n",
            "Accuracy for 2-laplacian: 0.7128968029578078\n",
            "The maximum accuracy for 0 laplacian is: 0.6809684959349593 and its index is: 0 with coefficient 0.47350807102581727\n",
            "The maximum accuracy for 1 laplacian is: 0.7060732298739089 and its index is: 0 with coefficient 0.47350807102581727\n",
            "The maximum accuracy for 2 laplacian is: 0.7128968029578078 and its index is: 0 with coefficient 0.47350807102581727\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import math\n",
        "\n",
        "Iteration=1\n",
        "random_numbers=[]\n",
        "accuracies_for_0_laplacian=[]\n",
        "accuracies_for_1_laplacian=[]\n",
        "accuracies_for_2_laplacian=[]\n",
        "\n",
        "while Iteration<=1:\n",
        "  %cd /content/Simplicial-Neural-Network-accuracy-improvement/data/s2_3_collaboration_complex\n",
        "  laplacians = np.load('150250_laplacians.npy',allow_pickle=True)\n",
        "\n",
        "  random_number = random.random()\n",
        "  random_numbers.append(random_number)\n",
        "  line_length = 7  # Adjust this value based on how many elements we want in each line\n",
        "  counter = 0\n",
        "  for i in range(25466):\n",
        "    if laplacians[1].data[i]<0:\n",
        "      print(i,'. ',laplacians[1].data[i],end='->')\n",
        "      laplacians[1].data[i] = max(0.1 * laplacians[1].data[i], laplacians[1].data[i])\n",
        "      print(laplacians[1].data[i],end=', ')\n",
        "      counter+=1\n",
        "      if counter == line_length:\n",
        "        print()  # Print a new line\n",
        "        counter = 0  # Reset the counter\n",
        "  # for i in range(3285):\n",
        "  #   laplacians[1].data[(laplacians[1].row==i) & (laplacians[1].col==i)]+=1\n",
        "\n",
        "  %cd /content/Simplicial-Neural-Network-accuracy-improvement/data/s2_3_collaboration_complex\n",
        "  np.save('150250_laplacians.npy', laplacians)\n",
        "\n",
        "  %cd /content/Simplicial-Neural-Network-accuracy-improvement\n",
        "  !python ./experiments/impute_citations.py ./data/s2_3_collaboration_complex ./experiments/output 150250 30\n",
        "\n",
        "  #calculating accuracy for the 0-laplacian\n",
        "  %cd /content/Simplicial-Neural-Network-accuracy-improvement/experiments/output\n",
        "\n",
        "  folder_path = '/content/Simplicial-Neural-Network-accuracy-improvement/experiments/output'\n",
        "\n",
        "  files = os.listdir(folder_path)\n",
        "  files.sort()\n",
        "\n",
        "  prediction = []\n",
        "  actual = []\n",
        "\n",
        "  for filename in os.listdir('.'):\n",
        "    if filename.startswith('prediction') and filename.endswith('0.txt'):\n",
        "      with open(filename, 'r') as f:\n",
        "          numbers_str = f.read()\n",
        "      numbers_list = numbers_str.split()\n",
        "      numbers_int = list(map(float, numbers_list))\n",
        "      prediction += numbers_int\n",
        "    elif filename.startswith('actual') and filename.endswith('0.txt'):\n",
        "      with open(filename, 'r') as f:\n",
        "          numbers_str = f.read()\n",
        "      numbers_list = numbers_str.split()\n",
        "      numbers_int = list(map(float, numbers_list))\n",
        "      actual += numbers_int\n",
        "  prediction = [round(x, 0) for x in prediction]\n",
        "  actual = [round(x, 0) for x in actual]\n",
        "\n",
        "  accuracy = accuracy_score(actual, prediction)\n",
        "  accuracies_for_0_laplacian.append(accuracy)\n",
        "  print(\"We set this random number: \",random_number)\n",
        "  print(f'Accuracy for 0-laplacian: {accuracy}')\n",
        "\n",
        "  #calculating accuracy for the 1-laplacian\n",
        "  prediction = []\n",
        "  actual = []\n",
        "\n",
        "  for filename in os.listdir('.'):\n",
        "      if filename.startswith('prediction') and filename.endswith('1.txt'):\n",
        "          with open(filename, 'r') as f:\n",
        "              numbers_str = f.read()\n",
        "          numbers_list = numbers_str.split()\n",
        "          numbers_int = list(map(float, numbers_list))\n",
        "          prediction += numbers_int\n",
        "      elif filename.startswith('actual') and filename.endswith('1.txt'):\n",
        "          with open(filename, 'r') as f:\n",
        "              numbers_str = f.read()\n",
        "          numbers_list = numbers_str.split()\n",
        "          numbers_int = list(map(float, numbers_list))\n",
        "          actual += numbers_int\n",
        "  prediction = [round(x, 0) for x in prediction]\n",
        "  actual = [round(x, 0) for x in actual]\n",
        "\n",
        "  accuracy = accuracy_score(actual, prediction)\n",
        "  accuracies_for_1_laplacian.append(accuracy)\n",
        "  print(f'Accuracy for 1-laplacian: {accuracy}')\n",
        "\n",
        "  #calculating accuracy for the 2-laplacian\n",
        "  prediction = []\n",
        "  actual = []\n",
        "\n",
        "  for filename in os.listdir('.'):\n",
        "      if filename.startswith('prediction') and filename.endswith('2.txt'):\n",
        "          with open(filename, 'r') as f:\n",
        "              numbers_str = f.read()\n",
        "          numbers_list = numbers_str.split()\n",
        "          numbers_int = list(map(float, numbers_list))\n",
        "          prediction += numbers_int\n",
        "      elif filename.startswith('actual') and filename.endswith('2.txt'):\n",
        "          with open(filename, 'r') as f:\n",
        "              numbers_str = f.read()\n",
        "          numbers_list = numbers_str.split()\n",
        "          numbers_int = list(map(float, numbers_list))\n",
        "          actual += numbers_int\n",
        "  prediction = [round(x, 0) for x in prediction]\n",
        "  actual = [round(x, 0) for x in actual]\n",
        "\n",
        "  accuracy = accuracy_score(actual, prediction)\n",
        "  accuracies_for_2_laplacian.append(accuracy)\n",
        "  print(f'Accuracy for 2-laplacian: {accuracy}')\n",
        "\n",
        "  max_accuracy0 = max(accuracies_for_0_laplacian)\n",
        "  max_index0 = accuracies_for_0_laplacian.index(max_accuracy0)\n",
        "  print(f\"The maximum accuracy for 0 laplacian is: {max_accuracy0} and its index is: {max_index0} with coefficient {random_numbers[max_index0]}\")\n",
        "\n",
        "  max_accuracy1 = max(accuracies_for_1_laplacian)\n",
        "  max_index1 = accuracies_for_1_laplacian.index(max_accuracy1)\n",
        "  print(f\"The maximum accuracy for 1 laplacian is: {max_accuracy1} and its index is: {max_index1} with coefficient {random_numbers[max_index1]}\")\n",
        "\n",
        "  max_accuracy2 = max(accuracies_for_2_laplacian)\n",
        "  max_index2 = accuracies_for_2_laplacian.index(max_accuracy2)\n",
        "  print(f\"The maximum accuracy for 2 laplacian is: {max_accuracy2} and its index is: {max_index2} with coefficient {random_numbers[max_index2]}\")\n",
        "\n",
        "  Iteration+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syWnD1_LDwbB",
        "outputId": "3ac09788-b34a-4465-c820-81a4f53ad28a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0-laplacian accuracy:  0.6809684959349593 for random number:  0.47350807102581727\n"
          ]
        }
      ],
      "source": [
        "# 0-laplacian accuracies\n",
        "for i in range(len(accuracies_for_0_laplacian)):\n",
        "  print(\"0-laplacian accuracy: \",accuracies_for_0_laplacian[i],\"for random number: \",random_numbers[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-2nCKKuE0Kr",
        "outputId": "45839fb1-b0be-4030-f3b4-d1e2fb437826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-laplacian accuracy:  0.7060732298739089 for random number:  0.47350807102581727\n"
          ]
        }
      ],
      "source": [
        "# 1-laplacian accuracies\n",
        "for i in range(len(accuracies_for_1_laplacian)):\n",
        "  print(\"1-laplacian accuracy: \",accuracies_for_1_laplacian[i],\"for random number: \",random_numbers[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwYQsSE_E1Fj",
        "outputId": "7248490e-1e2f-44e1-c913-36744aeb2232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2-laplacian accuracy:  0.7128968029578078 for random number:  0.47350807102581727\n"
          ]
        }
      ],
      "source": [
        "# 2-laplacian accuracies\n",
        "for i in range(len(accuracies_for_2_laplacian)):\n",
        "  print(\"2-laplacian accuracy: \",accuracies_for_2_laplacian[i],\"for random number: \",random_numbers[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N386ALRTFaDQ",
        "outputId": "f466c62b-cf97-4e5a-f12d-7ceda2c1411a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max accuracy we got:  0.6809684959349593 0.7060732298739089 0.7128968029578078\n"
          ]
        }
      ],
      "source": [
        "print(\"Max accuracy we got: \",max_accuracy0,max_accuracy1,max_accuracy2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "authorship_tag": "ABX9TyPCr27z2XIs45+MUzhgOOm1",
      "include_colab_link": true,
      "name": "finding_optimal_laplacians-2-_coefficient_using_lrelu-0_1_epoch2000.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
